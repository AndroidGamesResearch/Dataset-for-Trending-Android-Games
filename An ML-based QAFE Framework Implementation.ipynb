{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "538a7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************\n",
    "#    Importing all necessary modules to run ML-based QAFE framework \n",
    "#***********************************************************************\n",
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from six import python_2_unicode_compatible\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#importing modules for Topic Modeling \n",
    "from sklearn.feature_extraction.text import CountVectorizer as countVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en import English\n",
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4de41434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************\n",
    "# Step 0: Inserting Android Apps User Reviews as input of the framework \n",
    "#***********************************************************************\n",
    "\n",
    "\n",
    "## We have Executed this same codes into 3 different dataset ; uncomment each dataset to test individual results\n",
    "\n",
    "#originalSource = pd.read_csv(\"Reviews1_TopFreeAndroidGames.csv\") #2265 User Remarks\n",
    "\n",
    "#originalSource = pd.read_csv(\"Reviews2_TopGrossingAndroidGames.csv\") #3960 User Remarks\n",
    "\n",
    "originalSource = pd.read_csv(\"Reviews3_TopPaidAndroidGames.csv\") #2198 User Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5520a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Cleaned_Reviews</th>\n",
       "      <th>Stopwords_Removed</th>\n",
       "      <th>Noise_Removed</th>\n",
       "      <th>Words_Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fun Game would like to know how the CPU is dec...</td>\n",
       "      <td>Fun Game would like to know how the CPU is dec...</td>\n",
       "      <td>Fun Game would like know CPU decided dice roll...</td>\n",
       "      <td>Fun Game like know CPU decided dice roll overa...</td>\n",
       "      <td>Fun Game like know CPU decided dice roll overa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The bots are at an unfair advantage! The game ...</td>\n",
       "      <td>The bots are at an unfair advantage The game i...</td>\n",
       "      <td>The bots unfair advantage The game well made u...</td>\n",
       "      <td>bots unfair advantage game well made unfair ev...</td>\n",
       "      <td>bot unfair advantage game well made unfair eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely LOVE this game, but it has not been...</td>\n",
       "      <td>Absolutely LOVE this game but it has not been ...</td>\n",
       "      <td>Absolutely LOVE game working past couple weeks...</td>\n",
       "      <td>Absolutely LOVE game working past couple weeks...</td>\n",
       "      <td>Absolutely LOVE game working past couple week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been unable to even get into the game for...</td>\n",
       "      <td>I ve been unable to even get into the game for...</td>\n",
       "      <td>I unable even get game weeks I sent reports he...</td>\n",
       "      <td>unable even get game weeks sent reports heard ...</td>\n",
       "      <td>unable even get game week sent report heard no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's a good game but doesn't have the live vid...</td>\n",
       "      <td>It s a good game but doesn t have the live vid...</td>\n",
       "      <td>It good game live video like advertising says ...</td>\n",
       "      <td>good game live video advertising Live video pl...</td>\n",
       "      <td>good game live video advertising Live video pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>Its a fun time passer but considering im payin...</td>\n",
       "      <td>Its a fun time passer but considering im payin...</td>\n",
       "      <td>Its fun time passer considering im paying play...</td>\n",
       "      <td>fun time passer considering im paying play id ...</td>\n",
       "      <td>fun time passer considering im paying play id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>Nothing like getting back to the classic Fruit...</td>\n",
       "      <td>Nothing like getting back to the classic Fruit...</td>\n",
       "      <td>Nothing like getting back classic Fruit Ninja ...</td>\n",
       "      <td>Nothing getting back classic Fruit Ninja alway...</td>\n",
       "      <td>Nothing getting back classic Fruit Ninja alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Bought the chainsaw sword and requested a refu...</td>\n",
       "      <td>Bought the chainsaw sword and requested a refu...</td>\n",
       "      <td>Bought chainsaw sword requested refund glitch ...</td>\n",
       "      <td>Bought chainsaw sword requested refund glitch ...</td>\n",
       "      <td>Bought chainsaw sword requested refund glitch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>This game is better than the free one. I bough...</td>\n",
       "      <td>This game is better than the free one I bought...</td>\n",
       "      <td>This game better free one I bought blades lost...</td>\n",
       "      <td>game better free bought blades lost abilities ...</td>\n",
       "      <td>game better free bought blade lost ability ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>Bought this game then immediately bought the c...</td>\n",
       "      <td>Bought this game then immediately bought the c...</td>\n",
       "      <td>Bought game immediately bought chainsaw blade ...</td>\n",
       "      <td>Bought game immediately bought chainsaw blade ...</td>\n",
       "      <td>Bought game immediately bought chainsaw blade ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2198 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Remarks  \\\n",
       "0     Fun Game would like to know how the CPU is dec...   \n",
       "1     The bots are at an unfair advantage! The game ...   \n",
       "2     Absolutely LOVE this game, but it has not been...   \n",
       "3     I've been unable to even get into the game for...   \n",
       "4     It's a good game but doesn't have the live vid...   \n",
       "...                                                 ...   \n",
       "2193  Its a fun time passer but considering im payin...   \n",
       "2194  Nothing like getting back to the classic Fruit...   \n",
       "2195  Bought the chainsaw sword and requested a refu...   \n",
       "2196  This game is better than the free one. I bough...   \n",
       "2197  Bought this game then immediately bought the c...   \n",
       "\n",
       "                                        Cleaned_Reviews  \\\n",
       "0     Fun Game would like to know how the CPU is dec...   \n",
       "1     The bots are at an unfair advantage The game i...   \n",
       "2     Absolutely LOVE this game but it has not been ...   \n",
       "3     I ve been unable to even get into the game for...   \n",
       "4     It s a good game but doesn t have the live vid...   \n",
       "...                                                 ...   \n",
       "2193  Its a fun time passer but considering im payin...   \n",
       "2194  Nothing like getting back to the classic Fruit...   \n",
       "2195  Bought the chainsaw sword and requested a refu...   \n",
       "2196  This game is better than the free one I bought...   \n",
       "2197  Bought this game then immediately bought the c...   \n",
       "\n",
       "                                      Stopwords_Removed  \\\n",
       "0     Fun Game would like know CPU decided dice roll...   \n",
       "1     The bots unfair advantage The game well made u...   \n",
       "2     Absolutely LOVE game working past couple weeks...   \n",
       "3     I unable even get game weeks I sent reports he...   \n",
       "4     It good game live video like advertising says ...   \n",
       "...                                                 ...   \n",
       "2193  Its fun time passer considering im paying play...   \n",
       "2194  Nothing like getting back classic Fruit Ninja ...   \n",
       "2195  Bought chainsaw sword requested refund glitch ...   \n",
       "2196  This game better free one I bought blades lost...   \n",
       "2197  Bought game immediately bought chainsaw blade ...   \n",
       "\n",
       "                                          Noise_Removed  \\\n",
       "0     Fun Game like know CPU decided dice roll overa...   \n",
       "1     bots unfair advantage game well made unfair ev...   \n",
       "2     Absolutely LOVE game working past couple weeks...   \n",
       "3     unable even get game weeks sent reports heard ...   \n",
       "4     good game live video advertising Live video pl...   \n",
       "...                                                 ...   \n",
       "2193  fun time passer considering im paying play id ...   \n",
       "2194  Nothing getting back classic Fruit Ninja alway...   \n",
       "2195  Bought chainsaw sword requested refund glitch ...   \n",
       "2196  game better free bought blades lost abilities ...   \n",
       "2197  Bought game immediately bought chainsaw blade ...   \n",
       "\n",
       "                                       Words_Lemmatized  \n",
       "0     Fun Game like know CPU decided dice roll overa...  \n",
       "1     bot unfair advantage game well made unfair eve...  \n",
       "2     Absolutely LOVE game working past couple week ...  \n",
       "3     unable even get game week sent report heard no...  \n",
       "4     good game live video advertising Live video pl...  \n",
       "...                                                 ...  \n",
       "2193  fun time passer considering im paying play id ...  \n",
       "2194  Nothing getting back classic Fruit Ninja alway...  \n",
       "2195  Bought chainsaw sword requested refund glitch ...  \n",
       "2196  game better free bought blade lost ability ver...  \n",
       "2197  Bought game immediately bought chainsaw blade ...  \n",
       "\n",
       "[2198 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#******************************************************************\n",
    "# Step 1 : Pre-Processing User Reviews \n",
    "#******************************************************************\n",
    "\n",
    "def unwantedCharacters_removing(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "    return text\n",
    "\n",
    "def stopwords_removing(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    text_tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_filtered= [word for word in text_tokens if not word in stop_words]\n",
    "    return (\" \").join(tokens_filtered)\n",
    "    \n",
    "def noise_removing(text):\n",
    "    blob = TextBlob(text)\n",
    "    string = ' '.join([word for (word,tag) in blob.tags if tag == \"NN\" or tag == \"NNS\" or tag == \"NNP\" or tag == \"NNPS\" or tag == \"JJ\" or tag == \"JJR\" or tag == \"JJS\" or tag == \"RB\" or tag == \"RBR\" or tag == \"RBS\" or tag == \"VB\" or tag == \"VBG\" or tag == \"VBD\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VPZ\" or tag == \"ADV\"or tag == \"VERB\"])\n",
    "    return string\n",
    "\n",
    "def words_lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "\n",
    "\n",
    "#initial cleaning of Reviews \n",
    "df_initialProcessing= pd.DataFrame(originalSource['Remarks']).dropna()\n",
    "\n",
    "df_initialProcessing['Cleaned_Reviews'] = df_initialProcessing['Remarks'].apply(unwantedCharacters_removing)\n",
    "df_initialProcessing['Stopwords_Removed'] = df_initialProcessing['Cleaned_Reviews'].apply(stopwords_removing)\n",
    "df_initialProcessing['Noise_Removed'] = df_initialProcessing['Stopwords_Removed'].apply(noise_removing)\n",
    "df_initialProcessing['Words_Lemmatized'] = df_initialProcessing['Noise_Removed'].apply(words_lemmatizing)\n",
    "\n",
    "df_initialProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "658f2114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_lemmatized</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Sentiment_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fun Game like know CPU decided dice roll overa...</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bot unfair advantage game well made unfair eve...</td>\n",
       "      <td>0.765476</td>\n",
       "      <td>-0.117857</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely LOVE game working past couple week ...</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unable even get game week sent report heard no...</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good game live video advertising Live video pl...</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>0.067532</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>fun time passer considering im paying play id ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>Nothing getting back classic Fruit Ninja alway...</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Bought chainsaw sword requested refund glitch ...</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>game better free bought blade lost ability ver...</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>Bought game immediately bought chainsaw blade ...</td>\n",
       "      <td>0.333730</td>\n",
       "      <td>-0.124603</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2198 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Words_lemmatized  Subjectivity_score  \\\n",
       "0     Fun Game like know CPU decided dice roll overa...            0.288889   \n",
       "1     bot unfair advantage game well made unfair eve...            0.765476   \n",
       "2     Absolutely LOVE game working past couple week ...            0.312500   \n",
       "3     unable even get game week sent report heard no...            0.540000   \n",
       "4     good game live video advertising Live video pl...            0.382418   \n",
       "...                                                 ...                 ...   \n",
       "2193  fun time passer considering im paying play id ...            0.200000   \n",
       "2194  Nothing getting back classic Fruit Ninja alway...            0.436667   \n",
       "2195  Bought chainsaw sword requested refund glitch ...            0.287500   \n",
       "2196  game better free bought blade lost ability ver...            0.460000   \n",
       "2197  Bought game immediately bought chainsaw blade ...            0.333730   \n",
       "\n",
       "      Polarity_score Sentiment_analysis  \n",
       "0          -0.033333           Negative  \n",
       "1          -0.117857           Negative  \n",
       "2          -0.037500           Negative  \n",
       "3          -0.540000           Negative  \n",
       "4           0.067532           Positive  \n",
       "...              ...                ...  \n",
       "2193        0.250000           Positive  \n",
       "2194        0.226667           Positive  \n",
       "2195       -0.012500           Negative  \n",
       "2196        0.180000           Positive  \n",
       "2197       -0.124603           Negative  \n",
       "\n",
       "[2198 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**********************************\n",
    "#Step 2: Apply Sentimental Analysis\n",
    "#**********************************\n",
    "\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(review): \n",
    "    return TextBlob(review).sentiment.subjectivity\n",
    "\n",
    "# function to calculate polarity\n",
    "def getPolarity(review):\n",
    "        return TextBlob(review).sentiment.polarity \n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "    \n",
    "\n",
    "#applying sentiment Analysis on Cleaned Reviews\n",
    "df_sentimentAnalysis = pd.DataFrame()\n",
    "df_sentimentAnalysis['Words_lemmatized'] = df_initialProcessing['Words_Lemmatized']\n",
    "df_sentimentAnalysis['Subjectivity_score'] = df_initialProcessing['Words_Lemmatized'].apply(getSubjectivity)\n",
    "df_sentimentAnalysis['Polarity_score'] = df_initialProcessing['Words_Lemmatized'].apply(getPolarity) \n",
    "df_sentimentAnalysis['Sentiment_analysis'] = df_sentimentAnalysis['Polarity_score'].apply(analysis)\n",
    "\n",
    "df_sentimentAnalysis[[\"Words_lemmatized\",\"Subjectivity_score\",\"Polarity_score\",\"Sentiment_analysis\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a175a8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_lemmatized</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Sentiment_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bot unfair advantage game well made unfair eve...</td>\n",
       "      <td>0.765476</td>\n",
       "      <td>-0.117857</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>put star going jail time row statistically imp...</td>\n",
       "      <td>0.691111</td>\n",
       "      <td>-0.223333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>play free late Microtransactions EVERYTHING no...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Neat adventure game thing need solve cleverly ...</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>install final fantasy Google Chromecast Androi...</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>playing hundred mobile game always favorite mi...</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>Unplayable zoomed way Would previously given s...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>great game classic many released today Unfortu...</td>\n",
       "      <td>0.602778</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>game still fantastic however control atrocious...</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>used leveling quest starfruit matter wish nost...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Words_lemmatized  Subjectivity_score  \\\n",
       "1     bot unfair advantage game well made unfair eve...            0.765476   \n",
       "16    put star going jail time row statistically imp...            0.691111   \n",
       "21    play free late Microtransactions EVERYTHING no...            0.633333   \n",
       "106   Neat adventure game thing need solve cleverly ...            0.658333   \n",
       "131   install final fantasy Google Chromecast Androi...            0.640000   \n",
       "...                                                 ...                 ...   \n",
       "2110  playing hundred mobile game always favorite mi...            0.634000   \n",
       "2114  Unplayable zoomed way Would previously given s...            0.633333   \n",
       "2130  great game classic many released today Unfortu...            0.602778   \n",
       "2137  game still fantastic however control atrocious...            0.720000   \n",
       "2192  used leveling quest starfruit matter wish nost...            1.000000   \n",
       "\n",
       "      Polarity_score Sentiment_analysis  \n",
       "1          -0.117857           Negative  \n",
       "16         -0.223333           Negative  \n",
       "21         -0.133333           Negative  \n",
       "106        -0.033333           Negative  \n",
       "131        -0.060000           Negative  \n",
       "...              ...                ...  \n",
       "2110       -0.016500           Negative  \n",
       "2114       -0.066667           Negative  \n",
       "2130       -0.005556           Negative  \n",
       "2137       -0.380000           Negative  \n",
       "2192       -0.500000           Negative  \n",
       "\n",
       "[140 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********************************************************************************************\n",
    "#Step 3: Separating Negative Reviews for Analysing what are users Negative Remarks abouts Apps\n",
    "#*********************************************************************************************\n",
    "\n",
    "df_NegativeReviews = pd.DataFrame(df_sentimentAnalysis.query(\"Sentiment_analysis == 'Negative'\"))\n",
    "\n",
    "#Seperating negative Reviews with Subjectivity greater than 0.6 \n",
    "df_mostExpressedNegativeReviews = pd.DataFrame(df_NegativeReviews.query(\"Subjectivity_score >= 0.6\"))\n",
    "\n",
    "df_mostExpressedNegativeReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49ae1566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_lemmatized</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Sentiment_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>online part game perfect playing singleplayer ...</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.185417</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>level hard goal work AND make level full versi...</td>\n",
       "      <td>0.605952</td>\n",
       "      <td>0.176190</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Not happy app Asks ace medium decline app work...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Sad say work app longer playable IOS supported...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Theres lot glitch memory tournament saving cool</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>Great Graphics Start stop whenever want Don on...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>Controls bit iffy work made amazing job GIANTS...</td>\n",
       "      <td>0.658974</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>beautiful game Doesn offer friggin ad Wonderfu...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>love game much really free version ad feel ori...</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>think good get refund good ol day easy play ze...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Words_lemmatized  Subjectivity_score  \\\n",
       "8     online part game perfect playing singleplayer ...            0.693750   \n",
       "40    level hard goal work AND make level full versi...            0.605952   \n",
       "43    Not happy app Asks ace medium decline app work...            1.000000   \n",
       "55    Sad say work app longer playable IOS supported...            0.600000   \n",
       "61      Theres lot glitch memory tournament saving cool            0.650000   \n",
       "...                                                 ...                 ...   \n",
       "2167  Great Graphics Start stop whenever want Don on...            0.916667   \n",
       "2170  Controls bit iffy work made amazing job GIANTS...            0.658974   \n",
       "2185  beautiful game Doesn offer friggin ad Wonderfu...            0.800000   \n",
       "2187  love game much really free version ad feel ori...            0.614286   \n",
       "2189  think good get refund good ol day easy play ze...            0.677778   \n",
       "\n",
       "      Polarity_score Sentiment_analysis  \n",
       "8           0.185417           Positive  \n",
       "40          0.176190           Positive  \n",
       "43          0.300000           Positive  \n",
       "55          0.100000           Positive  \n",
       "61          0.350000           Positive  \n",
       "...              ...                ...  \n",
       "2167        0.466667           Positive  \n",
       "2170        0.466667           Positive  \n",
       "2185        0.462500           Positive  \n",
       "2187        0.192857           Positive  \n",
       "2189        0.611111           Positive  \n",
       "\n",
       "[397 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*********************************************************************************************\n",
    "#Step 4: Separating Positive Reviews for Analysing what are users Positive Remarks abouts Apps\n",
    "#*********************************************************************************************\n",
    "\n",
    "df_PositiveReviews = pd.DataFrame(df_sentimentAnalysis.query(\"Sentiment_analysis == 'Positive'\"))\n",
    "\n",
    "#Seperating negative Reviews with Subjectivity greater than 0.6 \n",
    "df_mostExpressedPositiveReviews = pd.DataFrame(df_PositiveReviews.query(\"Subjectivity_score >= 0.6\"))\n",
    "\n",
    "df_mostExpressedPositiveReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb90f59f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word 0      Word 1      Word 2   Word 3   Word 4     Word 5  \\\n",
      "Topic 0    game       thing      really     star     hard       love   \n",
      "Topic 1    hard     control  impossible  version     play      phone   \n",
      "Topic 2    game     playing     control     play   mobile       love   \n",
      "Topic 3    game      boring        good     star     love    control   \n",
      "Topic 4    time       phone         far  control  version  difficult   \n",
      "Topic 5  mobile   difficult     version     play     game        way   \n",
      "Topic 6     fix  impossible        star     game     good       time   \n",
      "Topic 7     way        play        game    phone  control  difficult   \n",
      "Topic 8     far        play     control     game     good    version   \n",
      "Topic 9  really        love      mobile  playing      fix    control   \n",
      "\n",
      "             Word 6   Word 7      Word 8   Word 9  \\\n",
      "Topic 0   difficult     time      boring  version   \n",
      "Topic 1        time      way        love   boring   \n",
      "Topic 2       phone     good  impossible     time   \n",
      "Topic 3     playing    phone        hard  version   \n",
      "Topic 4        love     game      really     star   \n",
      "Topic 5        star    phone        time     hard   \n",
      "Topic 6         far  control       thing     hard   \n",
      "Topic 7  impossible      fix      really     hard   \n",
      "Topic 8         way     hard       thing     love   \n",
      "Topic 9   difficult      far         way     hard   \n",
      "\n",
      "                    Potential_keywords_for_labelling_terms  \n",
      "Topic 0             [game, love difficult, difficult time]  \n",
      "Topic 1  [control impossible, impossible version, versi...  \n",
      "Topic 2  [control play, play mobile, mobile love, love,...  \n",
      "Topic 3                            [good star, star, love]  \n",
      "Topic 4  [time, control, version difficult, difficult l...  \n",
      "Topic 5  [mobile, difficult version, version play, play...  \n",
      "Topic 6  [fix impossible, impossible star, star, game g...  \n",
      "Topic 7  [way play, play game, game, phone, control dif...  \n",
      "Topic 8  [play control, control, game good, good versio...  \n",
      "Topic 9              [love mobile, fix, control difficult]  \n",
      "['game', 'love difficult', 'difficult time']\n",
      "['control impossible', 'impossible version', 'version play', 'play phone', 'phone', 'time', 'way']\n",
      "['control play', 'play mobile', 'mobile love', 'love', 'phone good', 'good', 'impossible time', 'time']\n",
      "['good star', 'star', 'love']\n",
      "['time', 'control', 'version difficult', 'difficult love', 'love']\n",
      "['mobile', 'difficult version', 'version play', 'play game', 'game', 'way', 'star', 'phone']\n",
      "['fix impossible', 'impossible star', 'star', 'game good', 'good time', 'control']\n",
      "['way play', 'play game', 'game', 'phone', 'control difficult', 'difficult', 'impossible fix']\n",
      "['play control', 'control', 'game good', 'good version', 'version', 'thing', 'love']\n",
      "['love mobile', 'fix', 'control difficult']\n"
     ]
    }
   ],
   "source": [
    "#*******************************************************************************************************************\n",
    "#Step 5: Discovering Insights from User Reviews using Topic Modeling technique with LDA\n",
    "#*******************************************************************************************************************\n",
    " \n",
    "vectorizer = countVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}') \n",
    "data_vectorized = vectorizer.fit_transform(df_mostExpressedNegativeReviews['Words_lemmatized'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      max_iter=10,\n",
    "                                      learning_method='online',\n",
    "                                      random_state=100,\n",
    "                                      batch_size=128,\n",
    "                                      evaluate_every = -1,\n",
    "                                      n_jobs = -1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "\n",
    "LatentDirichletAllocation(batch_size=128, \n",
    "                          doc_topic_prior=None,\n",
    "                          evaluate_every=-1, \n",
    "                          learning_decay=0.7,\n",
    "                          learning_method=\"online\",\n",
    "                          learning_offset=10.0,\n",
    "                          max_doc_update_iter=1897, \n",
    "                          max_iter=10, \n",
    "                          mean_change_tol=0.001,\n",
    "                          n_components=10, \n",
    "                          n_jobs=-1, \n",
    "                          perp_tol=0.1,\n",
    "                          random_state=100,\n",
    "                          topic_word_prior=None,\n",
    "                          total_samples=1000000.0, \n",
    "                          verbose=0)\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 20], 'learning_decay': [0.5, 0.9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=10, learning_method='online', learning_offset=50.,random_state=0)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "\n",
    "GridSearchCV(cv=None, \n",
    "             error_score='raise',\n",
    "             estimator=LatentDirichletAllocation(batch_size=128, \n",
    "                                                 doc_topic_prior=None,\n",
    "                                                 evaluate_every=-1, \n",
    "                                                 learning_decay=0.7, \n",
    "                                                 learning_method=None,\n",
    "                                                 learning_offset=10.0, \n",
    "                                                 max_doc_update_iter=1897, \n",
    "                                                 max_iter=10,\n",
    "                                                 mean_change_tol=0.001,\n",
    "                                                 n_components=10, \n",
    "                                                 n_jobs=1,\n",
    "                                                 perp_tol=0.1,\n",
    "                                                 random_state=None,\n",
    "                                                 topic_word_prior=None,\n",
    "                                                 total_samples=1000000.0,\n",
    "                                                 verbose=0),\n",
    "        n_jobs=1,\n",
    "       param_grid={'n_components': [10, 20], \n",
    "                   'learning_decay': [0.5, 0.9]},\n",
    "             pre_dispatch='2*n_jobs', \n",
    "             refit=True, \n",
    "             return_train_score='warn',\n",
    "             scoring=None,\n",
    "             verbose=0)\n",
    "\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Create Document â€” Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(df_mostExpressedNegativeReviews['Words_lemmatized']))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic[\"dominant_topic\"] = dominant_topic\n",
    "\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "# View\n",
    "df_topic_keywords.head()\n",
    "#print(\"***\")\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "def tagging(topic_keywords):\n",
    "    for key in topic_keywords:\n",
    "        value = list(filter(None, topic_keywords[key])) # Get rid of empty items\n",
    "        Tagged = nltk.pos_tag(value)\n",
    "        return Tagged\n",
    "\n",
    "def labelTopics(text):\n",
    "    mylist = text\n",
    "    mylist = unwantedCharacters_removing(mylist)\n",
    "    mylist = stopwords_removing(mylist)\n",
    "    mylist = list(mylist.split(\" \"))\n",
    "    length = len(mylist)\n",
    "    topic = []\n",
    "\n",
    "#******************************************************************************************************************\n",
    "##Step 6:                                      RBLSALT Automated Labelling Algorthim \n",
    "#******************************************************************************************************************\n",
    "\n",
    "    # A = Noun\n",
    "    # B = Verb\n",
    "    # C = Adjective \n",
    "\n",
    "    for i in range(length):\n",
    "        doc = nlp(mylist[i])\n",
    "      \n",
    "\n",
    "        ####Rule 1: if A => B => C \n",
    "        # Where Noun is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        if(doc[0].tag_ == 'NN'): # if the elemnet is Noun\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1]) # find the adjacent element \n",
    "\n",
    "             ##Rule 1.1: if A => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is also Noun \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ elements\n",
    "\n",
    "             ##Rule 1.2: if A => B    \n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is verb \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Noun + Verb (Game Stop)\n",
    "                topic.append(string)\n",
    "\n",
    "             ##Rule 1.3: if A => C \n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is Adjective \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Noun + Adjective \n",
    "                topic.append(string)\n",
    "\n",
    "\n",
    "        ####Rule 2: if B => A => C\n",
    "         # Where Verb is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        elif(doc[0].tag_ == 'VB'): # if the elemnet is Verb\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1])  # find the adjacent element \n",
    "\n",
    "             ##Rule 2.1: if B => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is Noun \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Verb + Noun \n",
    "                topic.append(string) \n",
    "\n",
    "             ##Rule 2.2: if B => B    \n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is also verb \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ element\n",
    "\n",
    "             ##Rule 2.3: if B => C    \n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is Adjective \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Verb + Adjective\n",
    "                topic.append(string)\n",
    "                \n",
    "                \n",
    "\n",
    "         ####Rule 3: if C => A => B\n",
    "         # Where Adjective is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        elif(doc[0].tag_ == 'JJ'): # if the elemnet is Adjective\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1]) # find the adjacent element\n",
    "                    \n",
    "             ##Rule 3.1: if C => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is Noun\n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Adjective + Noun \n",
    "                topic.append(string) \n",
    "\n",
    "             ##Rule 3.2: if C => B\n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is verb \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Adjective + Verb \n",
    "                topic.append(string)\n",
    "\n",
    "             ##Rule 3.3: if C => C\n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is also adjective  \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ element\n",
    "       \n",
    "    return topic\n",
    "\n",
    "#********************************* RBLSALT ends here **********************************************\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)   \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "\n",
    "values = ' '.join([str(i) for i in df_topic_keywords.values.tolist()])\n",
    "\n",
    "topic1_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic2_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic3_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic4_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic5_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic6_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic7_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic8_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic9_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic10_values = values.split(\"]\",1)[0]\n",
    "\n",
    "\n",
    "topic_1 = labelTopics(topic1_values)\n",
    "topic_2 = labelTopics(topic2_values)\n",
    "topic_3 = labelTopics(topic3_values)\n",
    "topic_4 = labelTopics(topic4_values)\n",
    "topic_5 = labelTopics(topic5_values)\n",
    "topic_6 = labelTopics(topic6_values)\n",
    "topic_7 = labelTopics(topic7_values)\n",
    "topic_8 = labelTopics(topic8_values)\n",
    "topic_9 = labelTopics(topic9_values)\n",
    "topic_10 = labelTopics(topic10_values)\n",
    "\n",
    "\n",
    "\n",
    "Topics = [topic_1,topic_2,topic_3,topic_4,topic_5,topic_6,topic_7,topic_8,topic_9,topic_10]\n",
    "    \n",
    "df_topic_keywords['Potential_keywords_for_labelling_terms']= Topics\n",
    "print(df_topic_keywords)\n",
    "print(topic_1)\n",
    "print(topic_2)\n",
    "print(topic_3)\n",
    "print(topic_4)\n",
    "print(topic_5)\n",
    "print(topic_6)\n",
    "print(topic_7)\n",
    "print(topic_8)\n",
    "print(topic_9)\n",
    "print(topic_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4cba036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word 0 Word 1      Word 2     Word 3      Word 4     Word 5  \\\n",
      "Topic 0     great    app        want      start         far       play   \n",
      "Topic 1    played   hour         buy       wish       phone  fantastic   \n",
      "Topic 2   version   work     amazing        job      mobile        bit   \n",
      "Topic 3      best    far         got     really         fun    graphic   \n",
      "Topic 4      nice   game  controller    support     amazing       star   \n",
      "Topic 5      good   game        love      think    original   favorite   \n",
      "Topic 6   playing   good       music     puzzle        know        bit   \n",
      "Topic 7      game  great        time     puzzle        play     played   \n",
      "Topic 8      game   play        love    awesome  experience      great   \n",
      "Topic 9  emulator  issue      highly  recommend   excellent        use   \n",
      "\n",
      "              Word 6     Word 7      Word 8   Word 9  \\\n",
      "Topic 0          fix     change         bug     work   \n",
      "Topic 1        thing       game        year     free   \n",
      "Topic 2    excellent       year        cool    great   \n",
      "Topic 3   controller        use       happy    super   \n",
      "Topic 4       update       port      mobile  perfect   \n",
      "Topic 5       really      thing        free     play   \n",
      "Topic 6      amazing  challenge  definitely   bought   \n",
      "Topic 7  challenging       easy     perfect  graphic   \n",
      "Topic 8        phone       make         way      fun   \n",
      "Topic 9      control      thing     problem     used   \n",
      "\n",
      "                    Potential_keywords_for_labelling_terms  \n",
      "Topic 0  [great app, play fix, fix, change, bug work, w...  \n",
      "Topic 1  [hour buy, phone fantastic, fantastic thing, t...  \n",
      "Topic 2  [version work, work amazing, amazing job, job ...  \n",
      "Topic 3  [fun graphic, graphic controller, controller, ...  \n",
      "Topic 4  [nice game, game, controller, support amazing,...  \n",
      "Topic 5  [good game, game, love think, think original, ...  \n",
      "Topic 6  [good music, music puzzle, bit amazing, amazin...  \n",
      "Topic 7  [game great, great time, time puzzle, puzzle, ...  \n",
      "Topic 8  [game play, play love, love awesome, awesome e...  \n",
      "Topic 9  [emulator, recommend excellent, excellent use,...  \n",
      "['great app', 'play fix', 'fix', 'change', 'bug work', 'work']\n",
      "['hour buy', 'phone fantastic', 'fantastic thing', 'thing', 'game', 'year free', 'free']\n",
      "['version work', 'work amazing', 'amazing job', 'job mobile', 'mobile bit', 'bit excellent', 'excellent year', 'year cool', 'cool', 'great']\n",
      "['fun graphic', 'graphic controller', 'controller', 'use happy']\n",
      "['nice game', 'game', 'controller', 'support amazing', 'amazing star', 'star', 'update', 'port mobile', 'mobile', 'perfect']\n",
      "['good game', 'game', 'love think', 'think original', 'original', 'thing free', 'free play', 'play']\n",
      "['good music', 'music puzzle', 'bit amazing', 'amazing challenge']\n",
      "['game great', 'great time', 'time puzzle', 'puzzle', 'easy', 'perfect', 'graphic']\n",
      "['game play', 'play love', 'love awesome', 'awesome experience', 'experience great', 'great phone', 'phone make', 'make way', 'way', 'fun']\n",
      "['emulator', 'recommend excellent', 'excellent use', 'use', 'control', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Again performing this step for Positive Remarks\n",
    "#*******************************************************************************************************************\n",
    "# n Step 5: Discovering Insights from User Reviews using Topic Modeling technique with LDA\n",
    "#*******************************************************************************************************************\n",
    " \n",
    "vectorizer = countVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}') \n",
    "data_vectorized = vectorizer.fit_transform(df_mostExpressedPositiveReviews['Words_lemmatized'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      max_iter=10,\n",
    "                                      learning_method='online',\n",
    "                                      random_state=100,\n",
    "                                      batch_size=128,\n",
    "                                      evaluate_every = -1,\n",
    "                                      n_jobs = -1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "\n",
    "LatentDirichletAllocation(batch_size=128, \n",
    "                          doc_topic_prior=None,\n",
    "                          evaluate_every=-1, \n",
    "                          learning_decay=0.7,\n",
    "                          learning_method=\"online\",\n",
    "                          learning_offset=10.0,\n",
    "                          max_doc_update_iter=1897, \n",
    "                          max_iter=10, \n",
    "                          mean_change_tol=0.001,\n",
    "                          n_components=10, \n",
    "                          n_jobs=-1, \n",
    "                          perp_tol=0.1,\n",
    "                          random_state=100,\n",
    "                          topic_word_prior=None,\n",
    "                          total_samples=1000000.0, \n",
    "                          verbose=0)\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 20], 'learning_decay': [0.5, 0.9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=10, learning_method='online', learning_offset=50.,random_state=0)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "\n",
    "GridSearchCV(cv=None, \n",
    "             error_score='raise',\n",
    "             estimator=LatentDirichletAllocation(batch_size=128, \n",
    "                                                 doc_topic_prior=None,\n",
    "                                                 evaluate_every=-1, \n",
    "                                                 learning_decay=0.7, \n",
    "                                                 learning_method=None,\n",
    "                                                 learning_offset=10.0, \n",
    "                                                 max_doc_update_iter=1897, \n",
    "                                                 max_iter=10,\n",
    "                                                 mean_change_tol=0.001,\n",
    "                                                 n_components=10, \n",
    "                                                 n_jobs=1,\n",
    "                                                 perp_tol=0.1,\n",
    "                                                 random_state=None,\n",
    "                                                 topic_word_prior=None,\n",
    "                                                 total_samples=1000000.0,\n",
    "                                                 verbose=0),\n",
    "        n_jobs=1,\n",
    "       param_grid={'n_components': [10, 20], \n",
    "                   'learning_decay': [0.5, 0.9]},\n",
    "             pre_dispatch='2*n_jobs', \n",
    "             refit=True, \n",
    "             return_train_score='warn',\n",
    "             scoring=None,\n",
    "             verbose=0)\n",
    "\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Create Document â€” Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(df_mostExpressedPositiveReviews['Words_lemmatized']))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic[\"dominant_topic\"] = dominant_topic\n",
    "\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "# View\n",
    "df_topic_keywords.head()\n",
    "#print(\"***\")\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "def tagging(topic_keywords):\n",
    "    for key in topic_keywords:\n",
    "        value = list(filter(None, topic_keywords[key])) # Get rid of empty items\n",
    "        Tagged = nltk.pos_tag(value)\n",
    "        return Tagged\n",
    "\n",
    "def labelTopics(text):\n",
    "    mylist = text\n",
    "    mylist = unwantedCharacters_removing(mylist)\n",
    "    mylist = stopwords_removing(mylist)\n",
    "    mylist = list(mylist.split(\" \"))\n",
    "    length = len(mylist)\n",
    "    topic = []\n",
    "\n",
    "    \n",
    "# Again performing this step for Positive Remarks\n",
    "#******************************************************************************************************************\n",
    "## Step 6:               RBLSALT Automated Labelling Algorthim \n",
    "#******************************************************************************************************************\n",
    "\n",
    "\n",
    "    # A = Noun\n",
    "    # B = Verb\n",
    "    # C = Adjective \n",
    "\n",
    "    for i in range(length):\n",
    "        doc = nlp(mylist[i])\n",
    "      \n",
    "\n",
    "        ####Rule 1: if A => B => C \n",
    "        # Where Noun is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        if(doc[0].tag_ == 'NN'): # if the elemnet is Noun\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1]) # find the adjacent element \n",
    "\n",
    "             ##Rule 1.1: if A => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is also Noun \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ elements\n",
    "\n",
    "             ##Rule 1.2: if A => B    \n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is verb \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Noun + Verb (Game Stop)\n",
    "                topic.append(string)\n",
    "\n",
    "             ##Rule 1.3: if A => C \n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is Adjective \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Noun + Adjective \n",
    "                topic.append(string)\n",
    "\n",
    "\n",
    "        ####Rule 2: if B => A => C\n",
    "         # Where Verb is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        elif(doc[0].tag_ == 'VB'): # if the elemnet is Verb\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1])  # find the adjacent element \n",
    "\n",
    "             ##Rule 2.1: if B => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is Noun \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Verb + Noun \n",
    "                topic.append(string) \n",
    "\n",
    "             ##Rule 2.2: if B => B    \n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is also verb \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ element\n",
    "\n",
    "             ##Rule 2.3: if B => C    \n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is Adjective \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Verb + Adjective\n",
    "                topic.append(string)\n",
    "                \n",
    "                \n",
    "\n",
    "         ####Rule 3: if C => A => B\n",
    "         # Where Adjective is followed by either Noun or Verb or Adjective\n",
    "\n",
    "        elif(doc[0].tag_ == 'JJ'): # if the elemnet is Adjective\n",
    "            if(i <= (length - 2)):\n",
    "                doc2 = nlp(mylist[i+1]) # find the adjacent element\n",
    "                    \n",
    "             ##Rule 3.1: if C => A\n",
    "            if(doc2[0].tag_ == 'NN'): # if adjacent element is Noun\n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Adjective + Noun \n",
    "                topic.append(string) \n",
    "\n",
    "             ##Rule 3.2: if C => B\n",
    "            if(doc2[0].tag_ == 'VB'): # if adjacent elemnt is verb \n",
    "                string = mylist[i] + \" \" + mylist[i+1] # Then concentarte the elements; i.e Adjective + Verb \n",
    "                topic.append(string)\n",
    "\n",
    "             ##Rule 3.3: if C => C\n",
    "            if(doc2[0].tag_ == 'JJ'): # if adjacent elemnt is also adjective  \n",
    "                topic.append(mylist[i]) # Then both of the elements are independ element\n",
    "       \n",
    "    return topic\n",
    "\n",
    "#********************************* RBLSALT ends here **********************************************\n",
    "\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)   \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "\n",
    "values = ' '.join([str(i) for i in df_topic_keywords.values.tolist()])\n",
    "\n",
    "topic1_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic2_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic3_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic4_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic5_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic6_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic7_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic8_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic9_values = values.split(\"]\",1)[0]\n",
    "values = values.split(\"]\",1)[1]\n",
    "topic10_values = values.split(\"]\",1)[0]\n",
    "\n",
    "\n",
    "topic_1 = labelTopics(topic1_values)\n",
    "topic_2 = labelTopics(topic2_values)\n",
    "topic_3 = labelTopics(topic3_values)\n",
    "topic_4 = labelTopics(topic4_values)\n",
    "topic_5 = labelTopics(topic5_values)\n",
    "topic_6 = labelTopics(topic6_values)\n",
    "topic_7 = labelTopics(topic7_values)\n",
    "topic_8 = labelTopics(topic8_values)\n",
    "topic_9 = labelTopics(topic9_values)\n",
    "topic_10 = labelTopics(topic10_values)\n",
    "\n",
    "\n",
    "Topics = [topic_1,topic_2,topic_3,topic_4,topic_5,topic_6,topic_7,topic_8,topic_9,topic_10]\n",
    "    \n",
    "df_topic_keywords['Potential_keywords_for_labelling_terms']= Topics\n",
    "print(df_topic_keywords)\n",
    "\n",
    "print(topic_1)\n",
    "print(topic_2)\n",
    "print(topic_3)\n",
    "print(topic_4)\n",
    "print(topic_5)\n",
    "print(topic_6)\n",
    "print(topic_7)\n",
    "print(topic_8)\n",
    "print(topic_9)\n",
    "print(topic_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cd875",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
